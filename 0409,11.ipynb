{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.247937\n",
      "200 0.53398067\n",
      "400 0.42206126\n",
      "600 0.34261703\n",
      "800 0.26828223\n",
      "1000 0.22655834\n",
      "1200 0.20543817\n",
      "1400 0.1879079\n",
      "1600 0.17309071\n",
      "1800 0.16039485\n",
      "2000 0.14939515\n",
      "--------------\n",
      "[[1.9919185e-03 9.9799699e-01 1.1013929e-05]] [1]\n",
      "--------------\n",
      "[[0.9086645  0.07732824 0.01400719]] [0]\n",
      "--------------\n",
      "[[1.1376309e-08 3.2811754e-04 9.9967194e-01]] [2]\n",
      "--------------\n",
      "[[1.9919185e-03 9.9799699e-01 1.1013929e-05]\n",
      " [9.0866452e-01 7.7328242e-02 1.4007189e-02]\n",
      " [1.1376309e-08 3.2811754e-04 9.9967194e-01]] [1 0 2]\n"
     ]
    }
   ],
   "source": [
    "# Lab 6 Softmax Classifier\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "X = tf.placeholder(\"float\", [None, 4])\n",
    "Y = tf.placeholder(\"float\", [None, 3])\n",
    "nb_classes = 3\n",
    "\n",
    "W = tf.Variable(tf.random_normal([4, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "            _, cost_val = sess.run([optimizer, cost], feed_dict={X: x_data, Y: y_data})\n",
    "\n",
    "            if step % 200 == 0:\n",
    "                print(step, cost_val)\n",
    "\n",
    "    print('--------------')\n",
    "    # Testing & One-hot encoding\n",
    "    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
    "    print(a, sess.run(tf.argmax(a, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
    "    print(b, sess.run(tf.argmax(b, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
    "    print(c, sess.run(tf.argmax(c, 1)))\n",
    "\n",
    "    print('--------------')\n",
    "    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
    "    print(all, sess.run(tf.argmax(all, 1)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 16) (96, 1)\n",
      "(5, 16) (5, 1)\n",
      "one_hot: Tensor(\"one_hot_6:0\", shape=(?, 1, 7), dtype=float32)\n",
      "reshape one_hot: Tensor(\"Reshape_6:0\", shape=(?, 7), dtype=float32)\n",
      "Step:     0\tCost: 8.578\tAcc: 4.17%\n",
      "Step:   100\tCost: 0.649\tAcc: 84.38%\n",
      "Step:   200\tCost: 0.416\tAcc: 88.54%\n",
      "Step:   300\tCost: 0.306\tAcc: 90.62%\n",
      "Step:   400\tCost: 0.242\tAcc: 92.71%\n",
      "Step:   500\tCost: 0.199\tAcc: 94.79%\n",
      "Step:   600\tCost: 0.169\tAcc: 97.92%\n",
      "Step:   700\tCost: 0.147\tAcc: 97.92%\n",
      "Step:   800\tCost: 0.130\tAcc: 97.92%\n",
      "Step:   900\tCost: 0.117\tAcc: 98.96%\n",
      "Step:  1000\tCost: 0.106\tAcc: 98.96%\n",
      "Step:  1100\tCost: 0.098\tAcc: 98.96%\n",
      "Step:  1200\tCost: 0.090\tAcc: 98.96%\n",
      "Step:  1300\tCost: 0.084\tAcc: 98.96%\n",
      "Step:  1400\tCost: 0.078\tAcc: 100.00%\n",
      "Step:  1500\tCost: 0.074\tAcc: 100.00%\n",
      "Step:  1600\tCost: 0.069\tAcc: 100.00%\n",
      "Step:  1700\tCost: 0.066\tAcc: 100.00%\n",
      "Step:  1800\tCost: 0.062\tAcc: 100.00%\n",
      "Step:  1900\tCost: 0.059\tAcc: 100.00%\n",
      "Step:  2000\tCost: 0.056\tAcc: 100.00%\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 5 True Y: 5\n",
      "[True] Prediction: 0 True Y: 0\n",
      "[True] Prediction: 6 True Y: 6\n",
      "[True] Prediction: 1 True Y: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nStep:     0 Loss: 5.106 Acc: 37.62%\\nStep:   100 Loss: 0.800 Acc: 79.21%\\nStep:   200 Loss: 0.486 Acc: 88.12%\\n...\\nStep:  1800\\tLoss: 0.060\\tAcc: 100.00%\\nStep:  1900\\tLoss: 0.057\\tAcc: 100.00%\\nStep:  2000\\tLoss: 0.054\\tAcc: 100.00%\\n[True] Prediction: 0 True Y: 0\\n[True] Prediction: 0 True Y: 0\\n[True] Prediction: 3 True Y: 3\\n...\\n[True] Prediction: 0 True Y: 0\\n[True] Prediction: 6 True Y: 6\\n[True] Prediction: 1 True Y: 1\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lab 6 Softmax Classifier\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "# Predicting animal type based on various features\n",
    "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[0:-5, 0:-1]\n",
    "y_data = xy[0:-5, [-1]]\n",
    "x_test = xy[-5:, 0:-1]\n",
    "y_test = xy[-5:, [-1]]\n",
    "print(x_data.shape, y_data.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "'''\n",
    "(101, 16) (101, 1)\n",
    "'''\n",
    "\n",
    "nb_classes = 7  # 0 ~ 6\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 16])\n",
    "Y = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6\n",
    "\n",
    "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
    "print(\"one_hot:\", Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
    "print(\"reshape one_hot:\", Y_one_hot)\n",
    "\n",
    "'''\n",
    "one_hot: Tensor(\"one_hot:0\", shape=(?, 1, 7), dtype=float32)\n",
    "reshape one_hot: Tensor(\"Reshape:0\", shape=(?, 7), dtype=float32)\n",
    "'''\n",
    "\n",
    "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
    "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
    "\n",
    "# tf.nn.softmax computes softmax activations\n",
    "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
    "logits = tf.matmul(X, W) + b\n",
    "hypothesis = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross entropy cost/loss\n",
    "\n",
    "#fist\n",
    "#cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=tf.stop_gradient([Y_one_hot])))\n",
    "#second\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y_one_hot * tf.log(hypothesis), axis=1))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "prediction = tf.argmax(hypothesis, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Launch graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for step in range(2001):\n",
    "        _, cost_val, acc_val = sess.run([optimizer, cost, accuracy], feed_dict={X: x_data, Y: y_data})\n",
    "                                        \n",
    "        if step % 100 == 0:\n",
    "            print(\"Step: {:5}\\tCost: {:.3f}\\tAcc: {:.2%}\".format(step, cost_val, acc_val))\n",
    "\n",
    "    # Let's see if we can predict\n",
    "    pred = sess.run(prediction, feed_dict={X: x_test})\n",
    "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
    "    for p, y in zip(pred, y_test.flatten()):\n",
    "        print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001, Cost: 2.567894222\n",
      "Epoch: 0002, Cost: 1.057073282\n",
      "Epoch: 0003, Cost: 0.852373454\n",
      "Epoch: 0004, Cost: 0.751388043\n",
      "Epoch: 0005, Cost: 0.686439356\n",
      "Epoch: 0006, Cost: 0.640169779\n",
      "Epoch: 0007, Cost: 0.605904839\n",
      "Epoch: 0008, Cost: 0.577114292\n",
      "Epoch: 0009, Cost: 0.553933755\n",
      "Epoch: 0010, Cost: 0.534603624\n",
      "Epoch: 0011, Cost: 0.517797843\n",
      "Epoch: 0012, Cost: 0.503041443\n",
      "Epoch: 0013, Cost: 0.489816915\n",
      "Epoch: 0014, Cost: 0.478698718\n",
      "Epoch: 0015, Cost: 0.467733410\n",
      "Learning finished\n",
      "Accuracy:  0.8898\n",
      "Label:  [9]\n",
      "Prediction:  [9]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lab 7 Learning rate and Evaluation\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "nb_classes = 10\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "# 0 - 9 digits recognition = 10 classes\n",
    "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
    "\n",
    "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
    "b = tf.Variable(tf.random_normal([nb_classes]))\n",
    "\n",
    "# Hypothesis (using softmax)\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# Test model\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "# Calculate accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "\n",
    "# parameters\n",
    "num_epochs = 15\n",
    "batch_size = 100\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize TensorFlow variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
    "\n",
    "    print(\"Learning finished\")\n",
    "\n",
    "    # Test the model using test sets\n",
    "    print(\n",
    "        \"Accuracy: \",\n",
    "        accuracy.eval(\n",
    "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Epoch: 0001, Cost: 2.826302672\n",
    "Epoch: 0002, Cost: 1.061668952\n",
    "Epoch: 0003, Cost: 0.838061315\n",
    "Epoch: 0004, Cost: 0.733232745\n",
    "Epoch: 0005, Cost: 0.669279885\n",
    "Epoch: 0006, Cost: 0.624611836\n",
    "Epoch: 0007, Cost: 0.591160344\n",
    "Epoch: 0008, Cost: 0.563868987\n",
    "Epoch: 0009, Cost: 0.541745171\n",
    "Epoch: 0010, Cost: 0.522673578\n",
    "Epoch: 0011, Cost: 0.506782325\n",
    "Epoch: 0012, Cost: 0.492447643\n",
    "Epoch: 0013, Cost: 0.479955837\n",
    "Epoch: 0014, Cost: 0.468893674\n",
    "Epoch: 0015, Cost: 0.458703488\n",
    "Learning finished\n",
    "Accuracy:  0.8951\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
