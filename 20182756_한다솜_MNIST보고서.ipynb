{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n처음에는 hidden layer를 2개만 하고 NN도 그다지 wide하게 하지 않았다. learning rate도 0.001로 했었다.\\n그래서 accuracy가 0.94정도로 나왔었다. \\nlayer 갯수를 늘리고 범위도 넓게 하고 learning rate도 약간 낮추었다. sigmoid를 relu로 바꾸고, GradientOpti~를 Adam으로 바꾸고, \\nrandom을 xavier initializer로 바꾸었다\\n또한 overfitting을 막기위해 dropout을 사용했더니 처리 속도는 늦어졌지만 아래와 같이 accuracy가 0.9833이 나왔다. 또한 cost값도 훨씬 작아졌다.\\n0.985가 나온 친구들은 레이어도 더 늘리고 범위도 넓게 하고 learning rate도 더 적게 했다는데 \\n처리속도가 너무 느려서 나는 이정도만 해도 굉장히 높아졌다고 생각한다.\\ntensorboard를 사용하려면 코드를 많이 고쳐야해서 오늘은 시간이 부족해서 못했다. \\n제출 후에 다시 자료를 보며 배워서 fashion mnist에 적용해봐야겠다.\\n\\n아래 코드에도 짤막하게 주석을 달아놓았다.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "처음에는 hidden layer를 2개만 하고 NN도 그다지 wide하게 하지 않았다. learning rate도 0.001로 했었다.\n",
    "그래서 accuracy가 0.94정도로 나왔었다. \n",
    "layer 갯수를 늘리고 범위도 넓게 하고 learning rate도 약간 낮추었다. sigmoid를 relu로 바꾸고, GradientOpti~를 Adam으로 바꾸고, \n",
    "random을 xavier initializer로 바꾸었다\n",
    "또한 overfitting을 막기위해 dropout을 사용했더니 처리 속도는 늦어졌지만 아래와 같이 accuracy가 0.9833이 나왔다. 또한 cost값도 훨씬 작아졌다.\n",
    "0.985가 나온 친구들은 레이어도 더 늘리고 범위도 넓게 하고 learning rate도 더 적게 했다는데 \n",
    "처리속도가 너무 느려서 나는 이정도만 해도 굉장히 높아졌다고 생각한다.\n",
    "tensorboard를 사용하려면 코드를 많이 고쳐야해서 오늘은 시간이 부족해서 못했다. \n",
    "제출 후에 다시 자료를 보며 배워서 fashion mnist에 적용해봐야겠다.\n",
    "\n",
    "아래 코드에도 짤막하게 주석을 달아놓았다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-35c58ea6fbd7>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\dasom\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\dasom\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\dasom\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\dasom\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\dasom\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  \n",
    "tf.reset_default_graph() #이 함수를 써주지 않으면 Xavier를 쓸 때 초기화가 안돼서 자꾸 경고문구가 뜬다\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "#Xavier initializer\n",
    "#슬라이드에 나와있는 대로 하면 자꾸 오류가 생겨서 따로 알려준 github 링크로 들어가서 그대로 따라했다\n",
    "def xavier_init(n_inputs, n_outputs, uniform=True):\n",
    "  if uniform:\n",
    "    init_range = math.sqrt(6.0 / (n_inputs + n_outputs))\n",
    "    return tf.random_uniform_initializer(-init_range, init_range)\n",
    "  else:\n",
    "    stddev = math.sqrt(3.0 / (n_inputs + n_outputs))\n",
    "    return tf.truncated_normal_initializer(stddev=stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#미리 각종 상수나 식들을 적어놓음\n",
    "learning_rate = 0.0007\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "dropout_rate = tf.placeholder(\"float\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dasom\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-4-d26b066550c7>:9: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-4-d26b066550c7>:35: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Accuracy:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope(\"Input_Layer\"):\n",
    "    X = tf.placeholder(tf.float32, [None, 784])\n",
    "    Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "with tf.name_scope(\"H_Layer1\"):\n",
    "    W1 = tf.get_variable(\"W1\", shape=[784, 512], initializer=xavier_init(784, 512))\n",
    "    B1 = tf.Variable(tf.random_normal([512]))\n",
    "    _L1 = tf.nn.relu(tf.add(tf.matmul(X,W1),B1))\n",
    "    L1 = tf.nn.dropout(_L1, dropout_rate)\n",
    "\n",
    "with tf.name_scope(\"H_Layer2\"):\n",
    "    W2 = tf.get_variable(\"W2\", shape=[512, 512], initializer=xavier_init(512, 400))\n",
    "    B2 = tf.Variable(tf.random_normal([512]))\n",
    "    _L2 = tf.nn.relu(tf.add(tf.matmul(L1, W2),B2)) # Hidden layer with ReLU activation\n",
    "    L2 = tf.nn.dropout(_L2, dropout_rate)\n",
    "\n",
    "with tf.name_scope(\"H_Layer3\"):    \n",
    "    W3 = tf.get_variable(\"W3\", shape=[512, 512], initializer=xavier_init(400, 256))\n",
    "    B3 = tf.Variable(tf.random_normal([512]))\n",
    "    _L3 = tf.nn.relu(tf.add(tf.matmul(L2, W3),B3)) # Hidden layer with ReLU activation\n",
    "    L3 = tf.nn.dropout(_L3, dropout_rate)\n",
    "\n",
    "with tf.name_scope(\"H_Layer4\"):\n",
    "    W4 = tf.get_variable(\"W4\", shape=[512, 256], initializer=xavier_init(256, 256))\n",
    "    B4 = tf.Variable(tf.random_normal([256]))\n",
    "    _L4 = tf.nn.relu(tf.add(tf.matmul(L3, W4),B4)) # Hidden layer with ReLU activation\n",
    "    L4 = tf.nn.dropout(_L4, dropout_rate)\n",
    "\n",
    "with tf.name_scope(\"OUTPUT\"):\n",
    "    W5 = tf.get_variable(\"W5\", shape=[256, 10], initializer=xavier_init(256, 10))\n",
    "    B5 = tf.Variable(tf.random_normal([10]))\n",
    "    hypothesis = tf.add(tf.matmul(L4, W5), B5)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "   logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "tf.summary.scalar(\"Cost\", cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar(\"Accuracy\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 0.565209198\n",
      "Epoch: 0002 cost = 0.194189459\n",
      "Epoch: 0003 cost = 0.144080599\n",
      "Epoch: 0004 cost = 0.116825159\n",
      "Epoch: 0005 cost = 0.101618007\n",
      "Epoch: 0006 cost = 0.085057558\n",
      "Epoch: 0007 cost = 0.075654451\n",
      "Epoch: 0008 cost = 0.068291243\n",
      "Epoch: 0009 cost = 0.061003829\n",
      "Epoch: 0010 cost = 0.055138744\n",
      "Epoch: 0011 cost = 0.051158181\n",
      "Epoch: 0012 cost = 0.047686544\n",
      "Epoch: 0013 cost = 0.047215570\n",
      "Epoch: 0014 cost = 0.041624546\n",
      "Epoch: 0015 cost = 0.040014842\n",
      "Learning Finished!\n",
      "Accuracy: 0.9822\n",
      "Label:  [3]\n",
      "Prediction:  [3]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "merged_summary = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter(\"./logs/test\")\n",
    "writer.add_graph(sess.graph)  # Show the graph\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, dropout_rate: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "print('Accuracy:',accuracy.eval(session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels, dropout_rate: 1}))\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], dropout_rate: 1}))\n",
    "\n",
    "plt.imshow(mnist.test.images[r:r + 1].reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
